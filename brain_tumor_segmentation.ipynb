{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9XwyMWuTTEcQ",
    "outputId": "f9993394-b6a0-47fa-bd12-740960d41696"
   },
   "source": [
    "# __This is a notebook of a PyTorch brain tumor semantic segmentation with Unet++ (ResNet101 backbone)!__ \n",
    "# Project by __[Nikita Bezukhov](https://github.com/NikitaBezukhov)__!\n",
    "### The data set is __[Brain MRI segmentation from Kaggle](https://www.kaggle.com/mateuszbuda/lgg-mri-segmentation)__.\n",
    "### Was built in __[Google Colab](https://colab.research.google.com/)__ environment, so make any adjustments needed for it to work on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "####\n",
    "####\n",
    "## __1. First lets install, import and define all necessary libraries/classes/functions that we will be using.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "outputs": [],
   "source": [
    "pip install barbar git+https://github.com/albumentations-team/albumentations segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "XHSa1bYySDhv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import segmentation_models_pytorch \n",
    "import logging\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import torchvision\n",
    "from imageio import imread\n",
    "from math import exp\n",
    "import torch.nn as nn\n",
    "from PIL import Image, ImageOps, ImageShow\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.metrics import jaccard_score\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, datasets \n",
    "from shutil import copyfile, move\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from barbar import Bar\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, auc\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fSzynDS6Sagm"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print,best_score=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = best_score\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when monitored metric decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Monitored metric has improved ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), f'/content/drive/MyDrive/brainmodelunetpp.pt') \n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "## __2. Data preparation.__\n",
    "### Dataset consists of MRI brain scans and ground truth masks.\n",
    "### We will unzip data, copy all scans and masks from different folders into 2 folders (masks, scans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fw2diKwUSach",
    "outputId": "fd3bc311-2d71-459f-d75a-99ab4a8f449a"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/brain_scans.zip -d /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2UiWajqQSaaP"
   },
   "outputs": [],
   "source": [
    "os.mkdir('/content/scans/')\n",
    "os.mkdir('/content/masks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YfvrTnEbZ8uT"
   },
   "outputs": [],
   "source": [
    "scans_dir = '/content/scans/'\n",
    "masks_dir = '/content/masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jV5ztJNWSaYV"
   },
   "outputs": [],
   "source": [
    " folders = os.listdir('/content/kaggle_3m/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BpGvSnzgSaWu"
   },
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    next_folder = '/content/kaggle_3m/'+folder\n",
    "    try:\n",
    "        for image in os.listdir(next_folder):\n",
    "            if 'mask' in image:\n",
    "                copyfile(os.path.join(next_folder, image), os.path.join(masks_dir, image)) \n",
    "            else:\n",
    "                copyfile(os.path.join(next_folder, image), os.path.join(scans_dir, image))   \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQ--S-HCSaUl",
    "outputId": "95980ee3-2fd4-4ad8-90c9-66944835bf0f"
   },
   "outputs": [],
   "source": [
    "print(len(os.listdir(scans_dir)))\n",
    "print(len(os.listdir(masks_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "### Now we will make lists of scan and mask names, sort them so masks and scans would be in the same order in lists, shuffle together and then split into train/val sets 85/15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cD77v4YiSaSc"
   },
   "outputs": [],
   "source": [
    "scan_list = os.listdir(scans_dir)\n",
    "mask_list = os.listdir(masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XLEAn3KejalJ"
   },
   "outputs": [],
   "source": [
    "scan_list.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "mask_list.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "omEU9MZVD2eH"
   },
   "outputs": [],
   "source": [
    "c = list(zip(scan_list, mask_list))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "scan_list, mask_list = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qmuq8ty4SaQ8",
    "outputId": "bdea8268-b94c-4498-aeaf-c0f8c1505a0c"
   },
   "outputs": [],
   "source": [
    "scan_list[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leQuJlDlSaOr",
    "outputId": "f02c7f3e-ef87-4e04-ef4d-2e6ee752a54c"
   },
   "outputs": [],
   "source": [
    "mask_list[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KYsq4CNTmfV8"
   },
   "outputs": [],
   "source": [
    "ind = len(scan_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9vUilIkvFSPX"
   },
   "outputs": [],
   "source": [
    "train_scan = scan_list[:int(ind*0.85)]\n",
    "train_mask = mask_list[:int(ind*0.85)]\n",
    "val_scan = scan_list[int(ind*0.85):]\n",
    "val_mask = mask_list[int(ind*0.85):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOmIw6EQGH6J",
    "outputId": "b13bccdd-dfe8-4474-83c2-0e7a28e739dd"
   },
   "outputs": [],
   "source": [
    "print(len(train_scan))\n",
    "print(len(train_mask))\n",
    "print(len(val_scan))\n",
    "print(len(val_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ir4daSQNz0p_"
   },
   "outputs": [],
   "source": [
    "path_scan_train_list = [scans_dir + x for x in train_scan]\n",
    "path_mask_train_list = [masks_dir + x for x in train_mask]\n",
    "path_scan_val_list = [scans_dir + x for x in val_scan]\n",
    "path_mask_val_list = [masks_dir + x for x in val_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_QRYhXKBiIv",
    "outputId": "00f27725-bbbe-45df-ac25-ce8a0e5b39be"
   },
   "outputs": [],
   "source": [
    "print(path_scan_train_list[0])\n",
    "print(path_mask_train_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "##\n",
    "## __3. Data augmentation.__\n",
    "### Dataset consists of 3929 unique MRI brain scans  of patients with and without brain tumor, image size is 256x256 pixels.\n",
    "### We will be using Albumentations image augmentation library to transform scans and corresponding masks in the same way.\n",
    "### I found next augmentations to make the most sense for this dataset: affine transformations with zoom 0.9-1.1, shift -0.1-0.1 and rotation -180-180 degrees. (arguments for zoom and shift were chosen pretty small so that brain tumor never gets out of picture)\n",
    "### We also incorporate random horizontal flip and normalization. (we will calculate mean and std for normalization later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "dcNLE9iFo7ME"
   },
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize (image_size, image_size, p=1.0),                       \n",
    "    A.augmentations.geometric.transforms.Affine(scale=(0.9,1.1), translate_percent={'x':(-0.1,0.1),'y':(-0.1,0.1)}, \n",
    "                                                rotate=(-180,180), shear=None, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "normalize_transforms = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.0896, 0.0813, 0.0864], std=[0.1342, 0.1232, 0.1288])\n",
    "    ])\n",
    "\n",
    "to_tensor_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize (image_size, image_size, p=1.0),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "### Next we make our custom dataset and specify order of all needed transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "3oqCy-7SmfI3"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_paths, target_paths, train=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.train = train\n",
    "\n",
    "    def transform(self, image, mask, train):\n",
    "        if train:\n",
    "            transformed = train_transforms(image=image, mask=mask)            \n",
    "            tensor_image = to_tensor_transforms(transformed['image'])\n",
    "            normalized_img = normalize_transforms(tensor_image)\n",
    "            tensor_mask = to_tensor_transforms(transformed['mask'])\n",
    "\n",
    "        else:      \n",
    "            transformed = val_transforms(image=image, mask=mask)   \n",
    "            tensor_image = to_tensor_transforms(transformed['image'])\n",
    "            normalized_img = normalize_transforms(tensor_image)\n",
    "            tensor_mask = to_tensor_transforms(transformed['mask'])\n",
    "\n",
    "\n",
    "        return normalized_img, tensor_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = imread(self.image_paths[index])\n",
    "        mask = imread(self.target_paths[index])\n",
    "        x, y = self.transform(image, mask, self.train)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "jayjogO5SaEn"
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(path_scan_train_list, path_mask_train_list, train=True)\n",
    "val_dataset = MyDataset(path_scan_val_list, path_mask_val_list, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "### Batch size chosen as 16 because it is all I could fit in 16gb Colab Pro GPU, but we will be using PyTorch gradient accumulation, so effectively we will have batch size of 64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "e9feAkg5SaCo"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True,\n",
    "    pin_memory=False, drop_last=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=16, shuffle=False,\n",
    "    pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "### Mean and std calculation for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qU9a7SQPGijh"
   },
   "outputs": [],
   "source": [
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "for inputs, label in train_loader:\n",
    "    psum    += inputs.sum(axis        = [0, 2, 3])\n",
    "    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])\n",
    "    \n",
    "# pixel count\n",
    "count = len(train_scan) * image_size * image_size\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: '  + str(total_mean))\n",
    "print('std:  '  + str(total_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "### We will be using Dice score and IoU as our metrics to measure model performance and Dice+BCE as a loss function. (from my experience addition of BCE loss helps model to start converging faster) \n",
    "### We divide loss by 4 because of usage of gradient accumulation (updating weights after every 4 minibatch loops). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Hy_v42BVK5jw"
   },
   "outputs": [],
   "source": [
    "def Dice_and_iou(inputs, targets, smooth=1e-6):\n",
    "    \n",
    "    #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "    # inputs = F.sigmoid(inputs)       \n",
    "    \n",
    "    #flatten label and prediction tensors\n",
    "    inputs = inputs.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    \n",
    "    intersection = (inputs * targets).sum()                            \n",
    "    dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "    iou = (intersection + smooth)/(inputs.sum() + targets.sum() - intersection + smooth)  \n",
    "    return dice, iou    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9mPmFVX6ALs2"
   },
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1e-6):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()  \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)                          \n",
    "        bce = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        \n",
    "        return ((1-dice)+bce)/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "## __4. Training.__\n",
    "### Model we will be using is Unet++ with ResNet101 backbone and ImageNet weights from __[here](https://github.com/qubvel/segmentation_models.pytorch)__!\n",
    "### We define it with 1 channel output for a mask.\n",
    "### I found combination of Adam optimizer with 0.001 starting learning rate, 0.0001 weight decay and ReduceLROnPlateau scheduler work the best. \n",
    "### Training process will be logged to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6Cv0pL2e_m60"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "model = segmentation_models_pytorch.UnetPlusPlus(encoder_name=\"resnet101\",in_channels=3, classes=1,\n",
    "                                                 aux_params=None, encoder_weights=\"imagenet\").to(device)  \n",
    "\n",
    "criterion = DiceBCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=10, cooldown=5, verbose=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir='/content/drive/MyDrive/brain_logs/', filename_suffix=\"brain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "### Early stopping was chosen as 50. It let model to get pretty small improvements in later stages of training. If you are not trying to squeeze every little bit of performance increase, patience of 30 would probably be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vyyod7K3SZ6l",
    "outputId": "dea2a9de-8cfb-4492-fddf-2c19d357dadb"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=50, verbose=True)\n",
    "batch_count = 0\n",
    "\n",
    "for epoch in range(999):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    all_train_iou = []\n",
    "    all_val_iou = []\n",
    "    all_train_dice = []\n",
    "    all_val_dice = []\n",
    "    print(f'Epoch {epoch+1}')\n",
    "\n",
    "    # Training loop\n",
    "    for idx, (inputs, masks) in enumerate(Bar(train_loader)):\n",
    "        batch_count += 1\n",
    "        model.train()\n",
    "        inputs, masks = inputs.to(device), masks.to(device)\n",
    "        outputs = model(inputs) \n",
    "        loss = criterion(torch.sigmoid(outputs), masks) \n",
    "        loss.backward() \n",
    "        if batch_count % 4 == 0:\n",
    "            optimizer.step() \n",
    "            optimizer.zero_grad() \n",
    "        train_loss.append(4*loss.item())\n",
    "        train_dice, train_iou = Dice_and_iou(torch.sigmoid(outputs).round(), masks)\n",
    "        all_train_iou.append(train_iou) \n",
    "        all_train_dice.append(train_dice) \n",
    "    all_train_iou = sum(all_train_iou)/(len(all_train_iou)+1e-6)\n",
    "    all_train_dice = sum(all_train_dice)/(len(all_train_dice)+1e-6)     \n",
    "    print(f\"Train IoU: {all_train_iou}\")\n",
    "    print(f\"Train Dice: {all_train_dice}\")\n",
    "    train_loss_final = sum(train_loss)/(len(train_loss)+1e-6)\n",
    "    scheduler.step(train_loss_final)\n",
    "    train_loss_formated = \"{:.4f}\".format(train_loss_final)\n",
    "\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in val_loader:\n",
    "            model.eval()           \n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(torch.sigmoid(outputs), masks) \n",
    "            val_loss.append(4*loss.item())\n",
    "            val_dice, val_iou = Dice_and_iou(torch.sigmoid(outputs.round()), masks)\n",
    "            all_val_iou.append(val_iou)\n",
    "            all_val_dice.append(val_dice)   \n",
    "    all_val_iou = sum(all_val_iou)/(len(all_val_iou)+1e-6)\n",
    "    all_val_dice = sum(all_val_dice)/(len(all_val_dice)+1e-6)\n",
    "    print(f\"Val IoU: {all_val_iou}\")   \n",
    "    print(f\"Val dice: {all_val_dice}\")    \n",
    "    val_loss_final = sum(val_loss)/(len(val_loss)+1e-6)\n",
    "    val_loss_formated = \"{:.4f}\".format(val_loss_final)\n",
    "    print(f'Training Loss: {train_loss_formated}')\n",
    "    print(f\"Validation Loss: {val_loss_formated}\")\n",
    "\n",
    "    # TensorBoard writer \n",
    "    writer.add_scalar('Loss/train', train_loss_final, epoch+1)\n",
    "    writer.add_scalar('Loss/val', val_loss_final, epoch+1)\n",
    "    writer.add_scalar('IoU/train', all_train_iou, epoch+1)\n",
    "    writer.add_scalar('IoU/val', all_val_iou, epoch+1)\n",
    "    writer.add_scalar('Dice/train', all_train_dice, epoch+1)\n",
    "    writer.add_scalar('Dice/val', all_val_dice, epoch+1)\n",
    "  \n",
    "    # Early Stopping\n",
    "    early_stopping(val_loss_final, model)       \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "# load the last checkpoint with the best model\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/brainmodelunetpp.pt'))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "##\n",
    "## __5. Results.__\n",
    "### Model stopped training after 312 epochs (around 9 hours on 1 Tesla V100 GPU) with Early stopping point at 262 epochs. \n",
    "### Improvements after first 4-5 hours of training were pretty marginal so if you are not trying to make as good of a model as you can, around 150 epochs should be enough.\n",
    "### Result metrics are:\n",
    "#### Train IoU: 0.8260\n",
    "#### Train Dice: 0.9006\n",
    "####\n",
    "#### Val IoU: 0.8407\n",
    "#### Val Dice: 0.9128\n",
    "####\n",
    "#### Training Loss: 0.1116\n",
    "#### Validation Loss: 0.0950\n",
    "####\n",
    "### Now we can make confusion per pixel classification confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "wXXNRdT4v6oH",
    "outputId": "ddf6060a-81ea-469b-a164-ca2ef61c9210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Healthy       1.00      1.00      1.00  38271128\n",
      "       Tumor       0.92      0.92      0.92    395112\n",
      "\n",
      "    accuracy                           1.00  38666240\n",
      "   macro avg       0.96      0.96      0.96  38666240\n",
      "weighted avg       1.00      1.00      1.00  38666240\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predlist = torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist = torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "predlistauc = torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(val_loader):\n",
    "        model.eval()\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist = torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist = torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "        predlistauc = torch.cat([predlistauc,torch.sigmoid(outputs).view(-1).cpu()])\n",
    "predlist = predlist.numpy()\n",
    "lbllist = lbllist.numpy()\n",
    "predlistauc = predlistauc.numpy()\n",
    "\n",
    "# Confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(lbllist, predlist).ravel()\n",
    "target_names = ['Healthy','Tumor']   \n",
    "ConfusionMatrixDisplay(np.concatenate((np.array([[tn, fp]]),np.array([[fn, tp]])),\n",
    "                                      axis=0), display_labels=target_names).plot(values_format=\"d\")                   \n",
    "print(classification_report(lbllist, predlist, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "### Next you can visualize scan/mask/prediction combinations to see results of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "jTacNHxBqnbf"
   },
   "outputs": [],
   "source": [
    "masks = torch.empty(1,1,image_size,image_size)\n",
    "modeled_masks = torch.empty(1,1,image_size,image_size)\n",
    "with torch.no_grad():\n",
    "    for inputs, mask in val_loader:\n",
    "        model.eval()       \n",
    "        inputs= inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        masks = torch.cat((masks,mask))\n",
    "        modeled_masks = torch.cat((modeled_masks,outputs.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "UtTCFBej1PE2"
   },
   "outputs": [],
   "source": [
    "normalize_transforms = transforms.Compose([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Fvv9DYshSZXl"
   },
   "outputs": [],
   "source": [
    "unnormalized_pics = torch.empty(1,3,image_size,image_size)\n",
    "with torch.no_grad():\n",
    "    for inputs, mask in val_loader:\n",
    "        unnormalized_pics = torch.cat((unnormalized_pics, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "A0_kzr2HSZVm"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for j in range(100):\n",
    "    count += 1\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax1.imshow(unnormalized_pics[j+1].permute(1,2,0))\n",
    "    ax1.set_title('Input Image', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax1.grid(False)\n",
    "\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax2.set_title('Ground Truth Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax2.imshow(masks[j+1].int().permute(1,2,0).squeeze(dim=2))\n",
    "    ax2.grid(False)\n",
    "\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    ax3.set_title('Predicted Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax3.imshow(torch.sigmoid(modeled_masks[j+1]).round().permute(1,2,0).squeeze(dim=2))\n",
    "    ax3.grid(False)\n",
    "\n",
    "    # plt.savefig('/content/drive/MyDrive/predictions/prediction_{}.png'.format(count), facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "source": [
    "### Also if we use our model as a classifier (look at how many tumors we predicted when there were none and backwards, etc.) we can calculate TP, TN, FP and FN, Recall, Precision and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "GBpOwLSRSZRq"
   },
   "outputs": [],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for i,mask in enumerate(masks[1:]):\n",
    "    if mask.max() == 1:\n",
    "        if torch.sigmoid(modeled_masks[i+1]).round().max() == 0:\n",
    "            FN += 1\n",
    "        if torch.sigmoid(modeled_masks[i+1]).round().max() == 1:  \n",
    "            TP += 1      \n",
    "    else:\n",
    "        if torch.sigmoid(modeled_masks[i+1]).round().max() == 1:  \n",
    "            FP += 1  \n",
    "        if torch.sigmoid(modeled_masks[i+1]).round().max() == 0:\n",
    "            TN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5dhpA_z941g",
    "outputId": "6a253a54-d3ff-4ca1-d5d0-e0ec9b8e75af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation as classification:\n",
      "\n",
      "TP: 202\n",
      "TN: 369\n",
      "FP: 7\n",
      "FN: 12\n"
     ]
    }
   ],
   "source": [
    "print('''Segmentation as classification:\n",
    "''')\n",
    "print('TP:',TP)\n",
    "print('TN:',TN)\n",
    "print('FP:',FP)\n",
    "print('FN:',FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "102kG45c_bbY"
   },
   "outputs": [],
   "source": [
    "recall = TP/(TP+FN)\n",
    "precision = TP/(TP+FP)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "accuracy = (TP+TN)/(TP+TN+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5hJ_RDzSZPs",
    "outputId": "367d3824-e277-48b8-f0d2-7b62eb634c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation as classification:\n",
      "\n",
      "Recall: 0.9439252336448598\n",
      "Precision: 0.9665071770334929\n",
      "F1 score: 0.9550827423167848\n",
      "Accuracy: 0.9677966101694915\n"
     ]
    }
   ],
   "source": [
    "print('''Segmentation as classification:\n",
    "''')\n",
    "print(f'Recall: {recall}')\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 score: {f1}\")\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFFEiBoCSZLk"
   },
   "source": [
    "### So this means that we predicted 94.4% out of all the scans with tumors (not necessarily each tumor in every scan) and 96.6% of scans we predicted to have tumor, had it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "993FMyCUSZKF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NlnYD4uSZID"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9YUS331GekN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxAwvXHDGVq8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Djzy1yu2GhYk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "brain_deep_lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
